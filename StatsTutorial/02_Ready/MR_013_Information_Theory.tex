 \documentclass[a4paper,12pt]{article}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\usepackage{eurosym}
\usepackage{vmargin}
\usepackage{amsmath}
\usepackage{multicol}
\usepackage{graphics}
\usepackage{epsfig}
\usepackage{enumerate}
\usepackage{framed}
\usepackage{subfigure}
\usepackage{fancyhdr}

\setcounter{MaxMatrixCols}{10}
%TCIDATA{OutputFilter=LATEX.DLL}
%TCIDATA{Version=5.00.0.2570}
%TCIDATA{<META NAME="SaveForMode" CONTENT="1">}
%TCIDATA{LastRevised=Wednesday, February 23, 2011 13:24:34}
%TCIDATA{<META NAME="GraphicsSave" CONTENT="32">}
%TCIDATA{Language=American English}

%\pagestyle{fancy}
\setmarginsrb{20mm}{0mm}{20mm}{25mm}{12mm}{11mm}{0mm}{11mm}
\lhead{Maths Resource} \rhead{Mathematics for Computing}
\chead{Information Theory }
%\input{tcilatex}

\begin{document}


%---------------------------------------------------------------- %
\newpage
\noindent {\Large \textbf{Information Theory Tutorial Sheet}}
\begin{enumerate}
\item Consider a source $X$ that produces five symbols with probabilities 1/2, 1/4, 1/8, 1/16 and 1/16. Determine the source entropy $H(x)$. 

\item A input source is a random variable X with a four letter alphabet $\{A,B,C,D\}$.
There are four different probability distributions presented below. Compute the entropy for each case.
\begin{center}
\begin{tabular}{|c c|c|c|c|c|}
\hline	&	$X_i$	&	A	&	B	&	C	&	D	\\ \hline
Case 1	&	$p(X_i)$	&	0.25	&	0.25	&	0.25	&	0.25	\\ \hline
Case 2	&	$p(X_i)$	&	0.25	&	0.5	&	0.125	&	0.125	\\ \hline
Case 3	&	$p(X_i)$	&	0.7	&	0.1	&	0.1	&	0.1	\\ \hline
Case 4	&	$p(X_i)$	&	0.97	&	0.01	&	0.01	&	0.01	\\ \hline
\end{tabular} 
\end{center}
\item 
Consider a source $X$ that produces 8 symbols with equal probabilities for each symbol. Determine the source entropy $H(x)$. 

    \item 
The input source to a noisy communication channel is a random variable X over three symbols a,b,c. The output from this channel is a random variable Y over the same three symbols. The joint distribution of these two random variables is as follows:


\begin{center}
\begin{tabular}{|c|c|c|c|}
\hline	 &x=a	&x=b&	     x=c\\ \hline
y=a	 &0.25	&0	 &    0.125 \\ \hline
y=b	 &0	    &0.125&	 0 \\ \hline
y=c	 &0.125&	0.25&	 0.125 \\ \hline
\end{tabular} 
\end{center}

\begin{enumerate}[(a)]
\item Write down the marginal distributions for X and Y. %(Last Week)
\item Compute the marginal entropies $H(X)$ and $H(Y)$ %(Last Week)
\item Compute the joint entropy $H(X,Y)$ of the two random variables. % (Last Week)
\item
Compute the mutual information $I(X;Y)$.
\item 
Compute the conditional entropies $H(X|Y)$ and $H(Y|X)$.
\item 
From Formulae: \[I(X,Y) = H(X) — H(X|Y)\]
\end{enumerate}

\item

A input source is a random variable X with a four letter alphabet $\{A,B,C,D\}$.
There are four different probability distributions presented below. 
Compute the entropy for each case.

\begin{center}
\begin{tabular}{|cc|c|c|c|}
\hline
	&	Case 1	&	Case 2	&	Case 3	&	Case 4	\\	
Xi	&	P(Xi)	&	P(Xi)	&	P(Xi)	&	P(Xi)	\\	\hline
A	&	0.25	&	0.25	&	0.7	&	0.97	\\	\hline
B	&	0.25	&	0.5	&	0.1	&	0.01	\\	\hline
C	&	0.25	&	0.125	&	0.1	&	0.01	\\	\hline
D	&	0.25	&	0.125	&	0.1	&	0.01	\\	\hline
\end{tabular} 
\end{center}
\item 
A four letter alphabet is encoded into binary form according to
\begin{center}
 \begin{tabular}{|c|c|c|c|c|}
\hline
Case 1	&	A:  10 	&	C:  110	&	G:  111	& T:  0 \\ \hline
Case 2	&	A:  00	&	C:  01	&		G: 10	&	T: 11 \\ \hline
\end{tabular}    
\end{center}

\begin{enumerate}[(a)]
    \item Using the code presented in case 1, decode the following sequence:	
\[11110001011010\]
\item Encode this message using the code from case 2. Compare the length of messages in both cases.
\end{enumerate}	



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\item 
Given that the alphabet has the following distribution 
\begin{center}
\begin{tabular}{|c|c|c|c|c|c|}
\hline
$x_i$	& A	& C	& G	& T \\ \hline
$p(x_i)$	& 0.25	& 0.125	& 0.125	& 0. 5 \\ \hline
\end{tabular} 
\end{center}
Compute the average symbol length for both cases.



%------------------------------------------------------------ %

%-----------------------------------------%
\item 
A DMS X has live symbols $\{x_l,x_2,x_3,x_4,x_5\}$ with $P(x_1) = 0.4$, $P(x_2)=0.19$, $P(x_3) =0.16$,
$P(x_4) = 0.15$, and $P(x_5) = 0.1$.
\begin{itemize}
\item[(a)] Construct the Shannon-Fano code for X, and calculate the efficiency of the code.
\item[(b)] Repeat for the Huffman code and compare the results.
\end{itemize}

\item A discrete memoryless source has a five symbol alphabet $\{x_1,x_2,x_3,x_4,x_5\}$ with the following probabilities 0.2,0.15,0.05,0.10 and 0.5.

\begin{itemize}
\item[(a)] Construct a Shannon-Fano code for $X$, and calculate the code efficiency.
\item[(b)] Construct a Huffman code for $X$, and calculate the code efficiency.
\end{itemize}

\item Consider a data source drawn from an alphabet (A, B, C, D) with probability
distribution $(0.3, 0.4, 0.2, 0.1)$.
\begin{enumerate}[(a)]
	\item Derive a fixed length binary code and a Huffman code for the source.
	\item Comment on the difference in shape of a binary tree representing the fixed
	length code and the Huffman tree for the source.
	\item Discuss which code is closer to the optimal. Show all your work to justify your
	answer.
\end{enumerate}
\item
A source language has 5 symbols A, B, C, D and E. The associated
probabilities of these symbols are given in the table below:
\begin{center}
	\begin{tabular}{|c|c|}
		\hline
		% after \\: \hline or \cline{col1-col2} \cline{col3-col4} ...
		Symbol & Probability \\ \hline
		A & 0.60 \\
		B & 0.30 \\
		C & 0.05 \\
		D & 0.03 \\
		E & 0.02 \\
		\hline
	\end{tabular}
\end{center}

\begin{enumerate}[(a)]
	\item Calculate the entropy of the source language.
	\item Define a Huffman binary code for the source language.
	\item Calculate the efficiency of the code in (b) above.
	\item Calculate the redundancy of the code in (b) above.
	\item Briefly state what is meant by the Prefix Condition.
\end{enumerate}

\item The frequency of 0 as an input to a binary channel is 0.6. If 0 is the input, then 0 is the output with probability 0.8. If 1 is the input, then 1 is the output with probability 0.9.

Write out the channel transition matrix

\begin{enumerate}[(a)]
\item	Calculate the output probabilities [P(Y)] 
\item	Compute the joint probabilities [P(X,Y)]
\item	Calculate the probability that the input is 0 given that the output is O. 
\item Calculate the probability that the input is l given that the output is 1, 
\item	Calculate the probability that the input is l given that the output is O.
\item	Calculate the probability that the input is 0 given that the output is 1. 
\end{enumerate}


  \item A file contains the following characters:
\begin{center}
	\begin{tabular}{|c|ccccccc|}
		\hline
		&&&&&&& \\[-0.4cm]
		$x_i$     & $x_1$ & $x_2$ & $x_3$ & $x_4$ & $x_5$ & $x_6$ & $x_7$ \\[0.1cm]
		\hline
		&&&&&&& \\[-0.4cm]
		$p(x_i)$  & 0.3 & 0.2 & 0.2 & 0.1 & 0.1 & 0.05 & 0.05 \\[0.1cm]
		\hline
		\multicolumn{7}{c}{}\\[-0.2cm]
	\end{tabular}
\end{center}

\begin{enumerate}[(a)]
    \item Calculate the entropy for this file. 
    \item Construct a Huffman code. 
    \item Calculate the expected length of the code. 
    \item Calculate the efficiency of the code.
\end{enumerate}

\item A file contains the following characters:
\begin{center}
	\begin{tabular}{|c|ccccc|}
		\hline
		&&&&& \\[-0.4cm]
		$x_i$     & a & b & c & d & e \\[0.1cm]
		\hline
		&&&&& \\[-0.4cm]
		$p(x_i)$  & 0.5 & 0.15 & 0.2 & 0.05 & 0.1 \\[0.1cm]
		\hline
		\multicolumn{6}{c}{}\\[-0.2cm]
	\end{tabular}
\end{center}
\begin{enumerate}[(i)]
    \item Calculate the entropy for this file. 
    \item Construct a Huffman code. 
    \item Calculate the expected length of the code. 
    \item  Calculate the efficiency of the code.
\end{enumerate}




\item 
A DMS X has live symbols $\{x_l,x_2,x_3,x_4,x_5\}$ with $P(x_1) = 0.2$, $P(x_2)=0.15$, $P(x_3) =0.05$,
$P(x_4) = 0.10$, and $P(x_5) = 0.5$. 
\begin{enumerate}[(a)]
\item Construct a Shannon—Fano code for X, and calculate the efficiency of the code.
\item Repeat for the Huffman code and compare the results.
\end{enumerate}

\item Given that the alphabet has the following distribution 
\begin{center}
    \begin{tabular}{|c|c|c|c|c|}
\hline
Xi & A & C & G & T \\ \hline
P(Xi) & 0.25 & 0.125 & 0.125 & 0. 5 \\
\hline
\end{tabular} 
\end{center}


Compute the average symbol length for both cases.


\item A file contains the following characters:\\[-0.1cm]
\begin{center}
	\begin{tabular}{|c|ccccccccccc|}
		\hline
		&&&&&&&&&&& \\[-0.4cm]
		$x_i$     & $x_1$ & $x_2$ & $x_3$ & $x_4$ & $x_5$ & $x_6$ & $x_7$  & $x_8$ & $x_9$  & $x_{10}$ & $x_{11}$ \\[0.1cm]
		\hline
		&&&&&&&&&&& \\[-0.4cm]
		$p(x_i)$  & 0.3 & 0.15 & 0.15 & 0.1 & 0.05 & 0.05 & 0.05 & 0.05 & 0.04 & 0.04 & 0.02 \\[0.1cm]
		\hline
		\multicolumn{12}{c}{}\\[-0.2cm]
	\end{tabular}
\end{center}

\begin{enumerate}[(a)]
	\item Construct a Huffman code.
\end{enumerate}


\item The frequency of 0 as an input to a binary channel is 0.6. If O is the
input, then 0 is the output with probability 0.8. If 1 is the input, then 1 is the output with probability 0.9.
\begin{enumerate}[(a)]
	\item Calculate the information per bit contained in the input.
	\item Calculate the probability that the output is 0.
	\item Calculate the probability that the output is 1.
	\item Calculate the probability that the input is 0 given that the
	output is O.
	\item Calculate the probability that the input is l given that
	the output is 1,
	\item Calculate the probability that the input is l given that
	the output is O.
	\item Calculate the probability that the input is 0 given that
	the output is l.
	\item Calculate the amount of information transmitted by the channel.
	\item Derive the globally optimal reconstruction rule.
\end{enumerate}
\end{enumerate}




\end{document}
