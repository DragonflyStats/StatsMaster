\documentclass[a4]{beamer}
\usepackage{amssymb}
\usepackage{graphicx}
\usepackage{subfigure}
\usepackage{newlfont}
\usepackage{amsmath,amsthm,amsfonts}
%\usepackage{beamerthemesplit}
\usepackage{pgf,pgfarrows,pgfnodes,pgfautomata,pgfheaps,pgfshade}
\usepackage{mathptmx} % Font Family
\usepackage{helvet} % Font Family
\usepackage{color}
\mode<presentation> {
\usetheme{Default} % was Frankfurt
\useinnertheme{rounded}
\useoutertheme{infolines}
\usefonttheme{serif}
%\usecolortheme{wolverine}
% \usecolortheme{rose}
\usefonttheme{structurebold}
}
\setbeamercovered{dynamic}
\title[MA4413]{Statistics for Computing \\ {\normalsize MA4413 Lecture 11A}}
\author[Kevin O'Brien]{Kevin O'Brien \\ {\scriptsize kevin.obrien@ul.ie}}
\date{Autumn 2011}
\institute[Maths \& Stats]{Dept. of Mathematics \& Statistics, \\ University \textit{of} Limerick}
\renewcommand{\arraystretch}{1.5}
%------------------------------------------------------------------------%

\begin{document}
	%---------------------------------------------------------------------------------------------------------------------------------------%
	
	\begin{frame}
	\frametitle{Channel Capacity}
	\begin{itemize} \item
		In information theory, channel capacity is the most conservative upper bound on the amount of information that can be reliably transmitted over a communications channel. \item  It is given by the maximum of the mutual information between the input and output of the channel (maximum in respect to input probabilities).
	\end{itemize}
	
\end{frame}
\begin{frame}
\frametitle{Channel Capacity}
\textbf{A. Channel Capacity per Symbol C:}\\
The channel capacity per symbol of a DMC is defined as
\[
C_s = \mbox{max }_{(P(x_i))}I(X; Y) \mbox{ b/symbol }
\]
where the maximization is over all possible input probability distributions $P(x_i)$ on X. Note that the
channel capacity $C_s$ is a function of only the channel transition probabilities that define the channel.

\end{frame}
%---------------------------------------------------------------------------------------------------------------------------------------%

\begin{frame}
\textbf{B. Channel Capacity per Second :}\\
If $r$ symbols are being transmitted per second, then the maximum rate of transmission of
information per second is $rC_s$.\\ This is the channel capacity per second and is denoted by $C$ (b/sec).
\[C = rC_s     \mbox{          b/sec} \]
\end{frame}

%------------------------------------------------------------------------%



%---------------------------------------------------------------------------------Page 251 C-%
\begin{frame}
\frametitle{Capacities of special channels}
\textbf{\emph{Lossless Channel}}\\\begin{itemize} \item For a lossless channel, the mutual information (information transfer) is equal to the input (source) entropy), and no source information is lost in transmission.\item It can be shown that $H(X|Y) = 0$ ( If $y_i$ is the output, there is certainty about the input). Also $I(X;Y) = H(X)$.
\item Consequently, the channel capacity per symbol is
\[ C_s = \mbox{ max }_{P(x_i)} H(X) = \mbox{log}_2m \]
where $m$ is the number of symbols in $X$.
\item For example, if there are $m=4$ input channels, then $C =  \mbox{log}_2 4 = 2$ b/symbol  \end{itemize}
\end{frame}

%---------------------------------------------------------------------------------Page 251 D-%
\begin{frame}
\frametitle{Capacities of special channels}
\textbf{\emph{Deterministic Channel}}:
\begin{itemize}
\item The mutual information (information transfer) is equal to the output entropy.
\item It can be shown that $H(Y|X) = 0$ ( If $x_i$ is the input, there is certainty about the output). Also $I(X;Y) = H(Y)$.
\item  The channel capacity per symbol is
\[ C_s = \mbox{ max }_{P(x_i)} H(Y) = \mbox{log}_2n \]
where $n$ is the number of symbols in $Y$.
\end{itemize}
\end{frame}
%---------------------------------------------------------------------------------Page 252 A -%
\begin{frame}
\frametitle{Capacities of special channels}
\textbf{\emph{Noiseless Channel}}:
\begin{itemize}
\item Since a noiseless channel is both lossless and deterministic , we can say that $I(X;Y) = H(X) = H(Y)$.
The mutual information (information transfer) is equal to the output entropy). \item The channel capacity per symbol is
\[ C_s = \mbox{log}_2m = \mbox{log}_2n \]
\end{itemize}
\end{frame}

%---------------------------------------------------------------------------------Page 252 A -%
\begin{frame}
\frametitle{Capacities of special channels}
\textbf{\emph{Binary Symmetric Channel}}:
\begin{itemize}
\item It can be shown that, for a binary symmetric channel, the the channel capacity per symbol is
\[ C_s = 1 + p\mbox{log}_2p  + 1-p\mbox{log}_2 (1-p)  \]
\end{itemize}
\end{frame}

\end{document}