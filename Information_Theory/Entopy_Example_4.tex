%----------------------------------------------------------------------------------%
\begin{frame}
\frametitle{Entropies: Example (a)}
\begin{itemize}
\item The input source to a noisy communication channel is a random variable X over the
four symbols $\{a, b, c, d\}$. \item  The output from this channel is a random variable Y over these same
four symbols.
\end{itemize}

\end{frame}
%----------------------------------------------------------------Part 2 %
\begin{frame}
\frametitle{Entropies: Example (b)}
The joint distribution of these two random variables is as follows:\\ \bigskip
\begin{center}
\begin{tabular}{|c|c|c|c|c|}
\hline
&x=a& x=b & x=c & x=d \\ \hline
y=a &1/8 &1/16 &1/16 &1/4 \\ \hline
y=b &1/16 & 1/8& 1/16& 0 \\ \hline
y=c & 1/32&1/32 & 1/16 & 0\\ \hline
y=d & 1/32& 1/32& 1/16 & 0\\ \hline 
\end{tabular}
\end{center}
\end{frame}
%----------------------------------------------------------------Part 2 %
\begin{frame}
\frametitle{Entropies: Example (c)}
\begin{itemize}
\item Write down the marginal distribution for $X$ and compute the marginal entropy $H(X)$.
\item Write down the marginal distribution for $Y$ and compute the marginal entropy $H(Y )$.
\item What is the joint entropy $H(X, Y ) $ of the two random variables?
\item What is the conditional entropy $H(Y|X)$?
\item What is the conditional entropy $H(X|Y)$?
\item What is the mutual information $I(X;Y)$ between the two random variables?
\end{itemize}
\end{frame}
%----------------------------------------------------------------------------------%
%----------------------------------------------------------------Part 2 %
\begin{frame}
\frametitle{Entropies: Example (d)}
The marginal distribution of these two random variables is as follows:\\ \bigskip
\begin{center}
\begin{tabular}{|c|c|c|c|c||c|}
\hline
&x=a& x=b & x=c & x=d &\alert{P(Y)}\\ \hline
y=a &1/8 &1/16 &1/16 &1/4 & \alert{0.50}\\ \hline
y=b &1/16 & 1/8& 1/16& 0 & \alert{0.25}\\ \hline
y=c & 1/32&1/32 & 1/16 & 0& \alert{0.125}\\ \hline
y=d & 1/32& 1/32& 1/16 & 0& \alert{0.125}\\ \hline \hline
\alert{P(X)} & \alert{0.25}& \alert{0.25}& \alert{0.25} & \alert{0.25}&\\ \hline
\end{tabular}
\end{center}
\end{frame}


\begin{frame}
\frametitle{Entropies: Example (e)}
\begin{itemize}

\item H(X) , the entropy of X, is computed as\\
 \[H(X) = -\sum P(x_i) \mbox{log}_2P(x_i)\] \item $H(X) =  (-0.25 \times -2) + (-0.25 \times -2) +(-0.25 \times -2) +(-0.25 \times -2)$\item $ H(X) = 2 \mbox{b}$ \bigskip

\item H(X) , the entropy of Y, is computed as\\
 \[H(Y) = -\sum P(y_j) \mbox{log}_2P(y_j)\] \item $H(Y) =  (-0.5 \times -1) +(-0.25 \times -2)  + (-0.125 \times -3)  +(-0.125 \times -3)$\item $ H(Y) = 1.75 \mbox{b}$



\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Entropies: Example (f)}
\begin{itemize}

\item To compute the joint entropy $H(X,Y)$,  we will use $H(X,Y) = -\sum P(x_i,y_y) \mbox{log}_2P(x_i,y_j)$
\bigskip
\item This means we should compute the entropy component for each cell of the table, and sum up all the resultant terms.
\bigskip
\item To save time, we will aggregate similar results, \begin{itemize} \item there are 4 cells where the probability is $1/32$,\item 6 cells with probability $1/16$, \item 2 cells with probability $1/8$ \item and 1 cell with probability $1/4$. \end{itemize}
\item Solving
\[ H(X,Y) = [4 \times -{1\over 32} \mbox{log}_2 {1\over 32} ] + [6 \times -{1\over 16} \mbox{log}_2 {1\over 16} ] + \ldots + [1 \times -{1\over 4} \mbox{log}_2 {1\over 4} ] \]
\end{itemize}
\end{frame}
%---------------------------------------------------------------------------------------------------------------------------------------%

\begin{frame}
\frametitle{Entropies: Example (g)}
\begin{itemize}
\item Simplifying\[ H(X,Y) = [4 \times -{1\over 32} \mbox{log}_2 {1\over 32} ] + [6 \times -{1\over 16} \mbox{log}_2 {1\over 16} ] + \ldots + [1 \times -{1\over 4} \mbox{log}_2 {1\over 4} ] \]
\item Simplifying \[H(X,Y) = [-{4\over 32} \times -5 ] + [-{6\over 16} \times -4 ] + [-{2\over 8} \times -3 ] + [ -{1\over 4} \times -2]\]
\item $H(X,Y) = 27/8$ b.
\end{itemize}
\end{frame}
%---------------------------------------------------------------------------------------------------------------------------------------%

\begin{frame}
\frametitle{Entropies: Example (h)}
From last lecture, two useful relationships among the types of entropies are
\begin{itemize}
\item $H(X,Y)=H(X|Y)+H(Y) $
\item $H(X,Y)=H(Y|X)+H(X) $
\end{itemize}
\bigskip
Re-arranging these formulae
\begin{itemize}
\item $H(X,Y)-H(Y) = H(X|Y) $
\item $H(X,Y)-H(X) = H(Y|X) $
\end{itemize}
\end{frame}

%---------------------------------------------------------------------------------------------------------------------------------------%

\begin{frame}
\frametitle{Entropies: Example (i)}
Re-arranging these formulae
\begin{itemize}
\item $H(X|Y) = H(X,Y)-H(Y) = 27/8 - 14/8 = 13/8$ b. \bigskip
\item $H(Y|X) = H(X,Y)-H(X) = 27/8 - 16/8 = 11/8$ b.
\end{itemize}
\bigskip
\begin{itemize}
\item Remark $1.75 =14/8$ and $2 = 16/8$.\\\bigskip
\item Also: we will derive $H(Y|X$ and $H(X|Y)$ from first principles in a tutorial.
\end{itemize}
\end{frame}
%---------------------------------------------------------------------------------------------------------------------------------------%

\begin{frame}
\frametitle{Entropies: Example (j)}
There are three alternative ways to obtain the answer:
\begin{itemize}
\item $I(X; Y ) = H(Y ) - H(Y |X)$ = $7/4 - 11/8 = 3/8$ b.
\item $I(X; Y ) = H(X) - H(X|Y)$ = $2 - 13/8 = 3/8$ b.
\item $I(X; Y ) = H(X) + H(Y ) - H(X,Y )$ = $2 + 7/4 - 27/8 = (16+14-27)/8 = 3/8$b.
\end{itemize}
\end{frame}
