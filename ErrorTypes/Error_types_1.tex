\documentclass[a4]{beamer}
\usepackage{amssymb}
\usepackage{graphicx}
\usepackage{subfigure}
\usepackage{newlfont}
\usepackage{amsmath,amsthm,amsfonts}
%\usepackage{beamerthemesplit}
\usepackage{pgf,pgfarrows,pgfnodes,pgfautomata,pgfheaps,pgfshade}
\usepackage{mathptmx}  % Font Family
\usepackage{helvet}   % Font Family
\usepackage{color}

\mode<presentation> {
 \usetheme{Default} % was Frankfurt
 \useinnertheme{rounded}
 \useoutertheme{infolines}
 \usefonttheme{serif}
 %\usecolortheme{wolverine}
% \usecolortheme{rose}
\usefonttheme{structurebold}
}

\setbeamercovered{dynamic}

\title[MA4413]{Statistics for Computing \\ {\normalsize MA4413 Lecture 8B}}
\author[Kevin O'Brien]{Kevin O'Brien \\ {\scriptsize Kevin.obrien@ul.ie}}
\date{Autumn Semester 2013}
\institute[Maths \& Stats]{Dept. of Mathematics \& Statistics, \\ University \textit{of} Limerick}

\renewcommand{\arraystretch}{1.5}

%------------------------------------------------------------------------%
\begin{document}
%--------------------------------------------------------------------------------------------------------------------------%
\begin{frame}
\frametitle{Hypothesis Testing}
\large
Recall: the inferential step to conclude that the null hypothesis is false goes as follows: The data (or data more extreme) are very unlikely given that the null hypothesis is true.
\bigskip
This means that:
\begin{itemize}\item [(1)] a very unlikely event occurred or
\item[(2)] the null hypothesis is false. \end{itemize}
The inference usually made is that the null hypothesis is false. Importantly it doesnt prove the null hypothesis to be false.
\end{frame}
%-------------------------------------------------------------------------------------------------------------------------%
\begin{frame}
\frametitle{Type I and II errors}
\large
There are two kinds of errors that can be made in hypothesis testing:
\begin{itemize}
\item[(1)] a true null hypothesis can be incorrectly rejected
\item[(2)] a false null hypothesis can fail to be rejected.
\end{itemize}
The former error is called a \textbf{\emph{Type I error}} and the latter error is called a \textbf{\emph{Type II error}}. \\ \bigskip
The probability of Type I error is always equal to the level of significance $\alpha$ (alpha) that is used as the standard for rejecting the null hypothesis .
\end{frame}
%---------------------------------------------------------------------------%
\begin{frame}
\frametitle{Type II Error}
\begin{itemize}

\item The probability of a Type II error is designated by the Greek letter beta ($\beta$).
\item A Type II error is only an error in the sense that an opportunity to reject the null hypothesis correctly was lost.
\item It is not an error in the sense that an incorrect conclusion was drawn since no conclusion is drawn when the null hypothesis is not rejected.
\end{itemize}
\end{frame}
%---------------------------------------------------------------------------%
\begin{frame}
\frametitle{Types of Error}
\large
\begin{itemize}
\item
A Type I error, on the other hand, is an error in every sense of the word. A conclusion is drawn that the null hypothesis is false when, in fact, it is true. \item Therefore, Type I errors are generally considered more serious than Type II errors.
\item
The probability of a Type I error ($\alpha$ ) is set by the experimenter. \item There is a trade-off between Type I and Type II errors. The more an experimenter protects himself or herself against Type I errors by choosing a low level, the greater the chance of a Type II error.
\end{itemize}
\end{frame}
%---------------------------------------------------------------------------%
\begin{frame}
\frametitle{Types of Error}
\large
\begin{itemize}
\item
Requiring very strong evidence to reject the null hypothesis makes it very unlikely that a true null hypothesis will be rejected. \item However, it increases the chance that a false null hypothesis will not be rejected, thus increasing the likelihood of Type II error.
\item
The Type I error rate is almost always set at 0.05 or at 0.01, the latter being more conservative since it requires stronger evidence to reject the null hypothesis at the 0.01 level then at the 0.05 level.
\item \textbf{Important}In this module, the significance level $\alpha$ can be assumed to be 0.05, unless explicitly stated otherwise.
\end{itemize}
\end{frame}
%---------------------------------------------------------------------------%
\begin{frame}
\frametitle{Type I and II errors}
\large
These two types of errors are defined in the table below.
\small
\begin{center}
\begin{tabular}{|c|c|c|}
\hline
&True State: H0 True & True State: H0 False\\\hline
Decision: Reject H0 & Type I error& Correct\\\hline
Decision: Do not Reject H0 & Correct &Type II error\\ \hline
\end{tabular}
\end{center}
\end{frame}

\begin{frame}
\frametitle{p-values}
\begin{itemize}
\item In hypothesis tests, the difference between the observed value and the parameter value specified by $H_0$ is computed and the probability of obtaining a difference this large or large is calculated.
\item The probability of obtaining data as extreme, or more extreme, than the expected value under the null hypothesis is called the \textbf{\emph{p-value}}.
\item It is not the probability of the null hypothesis itself.
\item Suppose if the probability value is $0.0175$, this does not mean that the probability that the null hypothesis is either true (or false) is $0.0175$.

\end{itemize}
\end{frame}
%--------------------------------------------------------------------------------------%
\begin{frame}
\frametitle{p-values}
\begin{itemize}
\item the p-value means that the probability of obtaining data as different or more different from the null hypothesis as those obtained in the experiment is $0.0175$.
\item If the p-value is less than the specified significance level, adjusted for the number of tails, then we reject the null hypothesis.
\[\mbox{ is p-value} \leq \frac{\alpha}{k} \mbox{?}\]
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{The Hypothesis Testing Procedure }
The second procedures is very similar to the first, but is more practicable for written exams, so we will use this one more. The first two steps are the same.

\begin{itemize}
\item Formally write out the null and alternative hypotheses (already described).
\item Compute the test statistic
\item Determine the \emph{\textbf{critical value}} (described shortly)
\item Make a decision based on the critical value.
\end{itemize}
\end{frame}


\begin{frame}
\frametitle{Hypothesis Tests for single samples}

\begin{itemize}
\item We could have inference procedures for single sample studies. We would base an argument on the either the sample mean or sample proportion as appropriate.
\item A hypothesis test can be used to determine how ``confident" we can be with our data in making that statements.
\item The lower the significance level (The margin for Type I error) the stronger our data must be.
\item Large samples lead to more confident conclusion.
\end{itemize}
\end{frame}
\begin{frame}
\frametitle{Hypothesis Tests for single samples}
\begin{itemize}
\item We could have either hypothesis test for the sample mean or the sample proportion, to test a statement about the population as a whole (i.e something about the population mean)
\item We make our argument in the form of the null and alternative hypotheses. 
\item The Hypothesis testing procedure determines the strength of evidence in making our arguments. 
\end{itemize}
\end{frame}
\begin{frame}
\frametitle{Hypothesis Tests for single samples}
\begin{itemize}
\item We simply follow the four step procedure. 
\item All of the components are the same used in confidence intervals.
\item The critical value is simply a quantile from the $Z$ or $t-$distribution.
\item The standard errors are also as before. Although when performing a hypothesis test for proportions, we use the expected value under the null hypothesis, rather than point estimate. (reason beyond scope of course.)
\end{itemize}
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%5


%----------------------------------------------------------------------------------%
\begin{frame}
\frametitle{Example 2: Paired Difference (a)}
\begin{itemize}
\item An automobile manufacturer collects mileage data for a sample of $n = 10$ cars in various weight categories
using a standard grade of gasoline with and without a particular additive. \item Of course, the engines were tuned to the same
specifications before each run, and the same drivers were used for the two gasoline conditions (with the driver in fact being
unaware of which gasoline was being used on a particular run). \item Given the mileage data on the next slide,  test the hypothesis
that there is no difference between the mean mileage obtained with and without the additive, using the 5 percent level of
significance \end{itemize}
\end{frame}
%-------------------------------------------------------------------------------------------%
\begin{frame}
\frametitle{Example 2: Paired Difference (b)}
\small
\begin{center}
\begin{tabular}{|c|c|c|c|c|}\hline
car & with additive & without additive & $d_i$ & $d^2_i$\\\hline
1&36.7&36.2&0.5&0.25\\\hline
2&35.8&35.7&0.1&0.01\\\hline
3&31.9&32.3&-0.4&0.16\\\hline
4&29.3&29.6&-0.3&0.09\\\hline
5&28.4&28.1&0.3&0.09\\\hline
6&25.7&25.8&-0.1&0.01\\\hline
7&24.2&23.9&0.3&0.09\\\hline
8&22.6&22.0&0.6&0.36\\\hline
9&21.9&21.5&0.4&0.16\\\hline
10&20.3&20.0&0.3&0.09\\\hline
\end{tabular}
\end{center}
\end{frame}

%--------------------------------------------------------------------------------------------------------------------------%
%-------------------------------------------------------------------------------------------%
\begin{frame}
\frametitle{Example 2: Paired Difference (c)}
\begin{itemize}
\item The average of the case wise differences is computed as \[\bar{d} = {\sum d_i \over n}\]
\[ \bar{d} = { 0.05 + 0.1  - 0.4 + \ldots + 0.30 \over 10 }= 0.17 \]
\item Also, using last column, $\sum d^2_i = (0.25 + 0.01 + 0.16 + \ldots + 0.09) = 1.31$
\end{itemize}

\end{frame}


\begin{frame}
\frametitle{Example 2: Paired Difference (d)}
\textbf{Sample standard deviation of the case-wise differences}:
\large
\[s_d = \sqrt{ {\sum d_i^2 - n\bar{d}^2 \over n-1}}\]
We know the following:
\begin{itemize}
\item The sample size $n$ which is 10.
\item The average of the case-wise differences. $\bar{d} = 0.17$
\item  $\sum d^2_i = 1.31$
\end{itemize}
\end{frame}



\begin{frame}
\frametitle{Example 2: Paired Difference (e)}
\textbf{Sample standard deviation  of the case-wise differences}://
\[s_d = \sqrt{ {\sum d_i^2 - n\bar{d}^2 \over n-1}}\]

\[s_d = \sqrt{ { 1.31 - 10(0.17)^2 \over 9}} = 0.337\]

\textbf{The standard error:} \[ S.E.(\bar{d}) = s_d / \sqrt{n} = {0.0337 \over 3.16} = 0.107\]
\end{frame}

\begin{frame}
\frametitle{Example 2: Paired Difference (f)}
\textbf{Null and Alternative Hypotheses}:
\begin{itemize}
\item That is, the null hypothesis is:\\
$H_0: \mu_d = 0$ Additive makes no difference to performance\\
$H_1: \mu_d \neq 0$ Additive makes a significant difference to performance \\
\end{itemize}
\textbf{Test Statistic}:
\[ TS = \frac{0.17 - 0}{0.107} = 1.59\]

\end{frame}
\end{document}
