
\section{Overview of experimental design}

Introduction
Analysis of variance (ANOVA) is a popular tool that has an applicability and
power that we can only start to appreciate in this course. The idea of analysis of
variance is to investigate how variation in structured data can be split into pieces associated
with components of that structure. We look only at one-way and two-way
classifications, providing tests and confidence intervals that are widely used in practice.

\begin{itemize}
\item Two-way ANOVA without interactions. \item Two-way ANOVA with
interactions.\item Two-way ANOVA with replicates \item Three-way
factorial design.
\end{itemize}


\section{MA4605: ANOVA}
We compute the test statistics $F = 62/3 \sim 20.7$ while the
$95\%$ quantile of F distribution with 3 and 8 degrees of freedom
is given as
\begin{verbatim}
>qf(0.95,3,8)
4.066181
\end{verbatim}

We clearly see that the test informs us about a significant
difference between the means. But which means are different?

The least significant difference method described in Section 3.9.

We compute the least significant difference $s \sqrt{2/n} \times
t$, where $s^{2}$ is within sample estimate of variance and $t$ is
the $97.5\%$ quantile of Student-$t$ distribution with $h(n-1)$
degrees of freedom.

\begin{verbatim}
>sqrt(mean(s))*sqrt(2/3)*qt(0.975,8)
# 3.261182
>m=apply(x,1,mean)
>m
#[1] 101 102 97 92
\end{verbatim}

The associated degrees of freedom: for within-sample $h(n - 1)$
(in our example $4 \times 2 = 8$), for between-sample $h - 1$ (in
our example 3). Total number of degrees freedom $hn-1$ and we see
$hn - 1 = h(n-1) + h - 1$.

But there is more then the relation between degrees of freedom.
Namely SST = SSM + SSR; where

WRONG
\begin{eqnarray}
SST = \sum_{j}\sum_{j}(x-\bar{x})^2\\
SSM= \sum_{j}\sum_{j}(x-\bar{x})^2\\
SSE = \sum_{j}\sum_{j}(x-\bar{x})^2\\
\end{eqnarray}


\begin{verbatim}
x=c(102,100,101,101,101,104,97,95,99,90,92,94)
factors=c(rep("A",3),rep("B",3),rep("C",3),rep("D",3))
res=aov(xfactors) anova(res)

Analysis of Variance Table Response:

x   Df Sum Sq Mean Sq   F value Pr(>F) factors 3 186 62
20.6670.0004002 *** Residuals 8 24 3
---
Signif. codes: 0 *** 0.001 ** 0.01 * 0.05 . 0.1 1
\end{verbatim}
\newpage

\section{Weighted Regression}

\textbf{Homoscedasticity} - the standard deviations of
y-observations from the straight line are the same independently
of the underlying x-observations.

\textbf{Heteroscedasticity} - the standard deviations of
y-observations depend on the underlying x-observations.

In the first case, standard regression analysis should be
performed, while in the second the weighted regression is more
suitable.

\begin{verbatim}
>Conc=c(0,2,4,6,8,10)
>StDev=c(0.001,0.004,0.010,0.013,0.017,0.022)
>Abs=c(0.009,0.158,0.301,0.472,0.577,0.739)
>n=length(Conc)
>weights=StDev(-2)/mean(StDev(-2))
>wreg=lm(AbsConc,weights=weights)
>reg=lm(AbsConc)
>summary(wreg)
\end{verbatim}


It is often convienent to express the regression analysis using
ANOVA table. The following equation is the basis for such
representation

It is often shortened to SST = SSLR + SSR; where SST is referred

to as the total sum of squares, SSLR is the sum of squares due to
linear regression (within regression), SSR is the sum of squares
due to residuals (outside regression).

\end{document}
